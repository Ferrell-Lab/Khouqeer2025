{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make virtual envrironment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m venv Juypterenv\n",
    "source Juypterenv/bin/activate\n",
    "pip install jinja2 MarkupSafe\n",
    "pip install umi_tools networkx pandas numpy matplotlib\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name=Juypterenv --display-name \"Python (Juypterenv)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "from umi_tools import network as nk\n",
    "import umi_tools as umi\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np, matplotlib.pyplot as plt, networkx as nx, pickle, json, gzip\n",
    "import subprocess\n",
    "import os\n",
    "import csv\n",
    "from collections import Counter\n",
    "import re\n",
    "import gzip, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all gzip fastq files\n",
    "dir_name = '/nobackup/l3_ferrell_lab/ABE/PRA_larry/fastq_files'\n",
    "\n",
    "def gz_extract(directory):\n",
    "    extension = '.gz'\n",
    "    os.chdir(directory)\n",
    "    for item in os.listdir(directory): # loop through items in dir\n",
    "      if item.endswith(extension): # check for '.gz' extension\n",
    "          gz_name = os.path.abspath(item) # get full path of files\n",
    "          file_name = (os.path.basename(gz_name)).rsplit('.',1)[0] #get file name for file within\n",
    "          with gzip.open(gz_name,'rb') as f_in, open(file_name,'wb') as f_out:\n",
    "              shutil.copyfileobj(f_in, f_out)\n",
    "          os.remove(gz_name) # delete zipped file\n",
    "        \n",
    "gz_extract(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run QC on Fastq files and check'em Not working \n",
    "data_directory = '/nobackup/l3_ferrell_lab/ABE/PRA_larry/fastq_files'\n",
    "subprocess.run([f'/nobackup/l3_ferrell_lab/ABE/PRA_larry/Juypterenv/FastQC/fastqc {data_directory}/*.fastq '], shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to wrangle Fastq files to include one sequence per line/row\n",
    "\n",
    "def extract_and_save_sequences(file_path, output_directory, N_READS=1):\n",
    "    '''\n",
    "    Extract sequences from a FASTQ file, filter barcodes using regex, count unique barcodes,\n",
    "    and save them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the FASTQ file.\n",
    "    - output_directory (str): The directory where the CSV file will be saved.\n",
    "    - N_READS (int): The minimum count of reads for a barcode to be included (default is 1).\n",
    "    '''\n",
    "    sequences = []\n",
    "    barcodes = []\n",
    "\n",
    "    barcode_pattern = '[ATCG]{4}TG[ATCG]{4}CA[ATCG]{4}AC[ATCG]{4}GA[ATCG]{4}GT[ATCG]{4}AG[ATCG]{4}'\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        # Extract the original file name without the extension\n",
    "        file_name = file_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "        # Loop through every 4 lines (header, sequence, separator, quality)\n",
    "        for i in range(0, len(lines), 4):\n",
    "            sequence = lines[i + 1].strip()  # Extract the sequence line\n",
    "            match = re.search(barcode_pattern, sequence)\n",
    "\n",
    "            if match:\n",
    "                barcode = match.group(0)\n",
    "                sequences.append(sequence)\n",
    "                barcodes.append(barcode)\n",
    "\n",
    "    # Count unique barcodes\n",
    "    barcode_counts = Counter(barcodes)\n",
    "\n",
    "    # Filter by the minimum count (N_READS)\n",
    "    filtered_barcodes = {barcode: count for barcode, count in barcode_counts.items() if count >= N_READS}\n",
    "\n",
    "    # Create a CSV file with barcodes and counts\n",
    "    output_file_name = f'Library_{file_name}_barcodes.csv'\n",
    "    output_file_path = os.path.join(output_directory, output_file_name)\n",
    "\n",
    "    with open(output_file_path, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['Barcode', 'Count'])  # Header\n",
    "        csv_writer.writerows(filtered_barcodes.items())\n",
    "\n",
    "    print(f'Filtered and counted barcodes saved to: {output_file_path}')\n",
    "\n",
    "\n",
    "\n",
    "def process_all_fastq_files(input_directory, output_directory, N_READS=1):\n",
    "    '''\n",
    "    Process all FASTQ files in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - input_directory (str): The path to the directory containing FASTQ files.\n",
    "    - output_directory (str): The directory where the CSV files will be saved.\n",
    "    - N_READS (int): The minimum count of reads for a barcode to be included (default is 1).\n",
    "    '''\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith('.fastq'):\n",
    "            file_path = os.path.join(input_directory, filename)\n",
    "            print(f'Processing file: {file_path}')\n",
    "            extract_and_save_sequences(file_path, output_directory, N_READS)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute functions on fastq directory\n",
    "data_directory = '/nobackup/l3_ferrell_lab/ABE/PRA_larry/fastq_files'\n",
    "input_directory = data_directory\n",
    "output_directory = data_directory\n",
    "process_all_fastq_files(input_directory, output_directory, N_READS=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collpase barcodes and generate a whitelist for each fastq file in directory\n",
    "\n",
    "def process_csv_files(input_directory, output_directory, threshold=5):\n",
    "    '''\n",
    "    Process all CSV files in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - input_directory (str): The path to the directory containing CSV files.\n",
    "    - output_directory (str): The directory where the output CSV files will be saved.\n",
    "    - threshold (int): The clustering threshold (default is 5).\n",
    "    '''\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_path = os.path.join(input_directory, filename)\n",
    "            print(f'Processing CSV file: {csv_path}')\n",
    "\n",
    "            # Read CSV file and extract barcodes and counts\n",
    "            cbFinal = dict()\n",
    "            with open(csv_path, 'r') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                next(reader)  # Skip header\n",
    "                for row in reader:\n",
    "                    barcode, count = row\n",
    "                    barcode = barcode.encode('utf-8')\n",
    "                    count = int(count)\n",
    "\n",
    "                    if barcode not in cbFinal:\n",
    "                        cbFinal[barcode] = 0\n",
    "\n",
    "                    cbFinal[barcode] += count\n",
    "\n",
    "            # Perform clustering\n",
    "            uc = nk.UMIClusterer()\n",
    "            CBclusters = uc(cbFinal, threshold=threshold)\n",
    "\n",
    "            # Create the final dictionary\n",
    "            cbFinalClustered = dict()\n",
    "            for cluster in CBclusters:\n",
    "                cbFinalClustered[cluster[0]] = sum(cbFinal[barcode] for barcode in cluster)\n",
    "\n",
    "            # Save the final clustered dictionary to a CSV file with the original CSV file name and 'whitelist'\n",
    "            csv_file_name = os.path.splitext(filename)[0]  # Extract CSV file name without extension\n",
    "            output_file_name = f'{csv_file_name}_whitelist.csv'\n",
    "            output_file_path = os.path.join(output_directory, output_file_name)\n",
    "\n",
    "            with open(output_file_path, 'w', newline='') as csvfile:\n",
    "                csv_writer = csv.writer(csvfile)\n",
    "                csv_writer.writerow(['Barcode', 'Count'])  # Header\n",
    "                for barcode, count in cbFinalClustered.items():\n",
    "                    barcode_str = barcode.decode('utf-8')  # Decode barcode from bytes to string\n",
    "                    csv_writer.writerow([barcode_str, count])\n",
    "\n",
    "            print(f'Final clustered whitelist saved to: {output_file_path}')\n",
    "\n",
    "\n",
    "\n",
    "process_csv_files(input_directory, output_directory, threshold = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering between the replicates to get reproducable whitelists via hamming distance. \n",
    "# These are for the dupilcates of each sample: \n",
    "\n",
    "def hamming_distance(bc1, bc2):\n",
    "    '''\n",
    "    Calculate the Hamming distance between two sequences.\n",
    "\n",
    "    Parameters:\n",
    "    - bc1 (str): First sequence.\n",
    "    - bc2 (str): Second sequence.\n",
    "\n",
    "    Returns:\n",
    "    - int: The Hamming distance between bc1 and bc2.\n",
    "    '''\n",
    "    # Ensure that the sequences are of equal length\n",
    "    if len(bc1) != len(bc2):\n",
    "        raise ValueError('Sequences must be of equal length for Hamming distance calculation.')\n",
    "\n",
    "    # Calculate Hamming distance\n",
    "    distance = np.sum([x1 != x2 for x1, x2 in zip(bc1, bc2)])\n",
    "\n",
    "    return distance\n",
    "\n",
    "def collapse_and_save(input_csv1, input_csv2, output_csv, HD):\n",
    "    '''\n",
    "    Collapse barcodes between datasets and within a dataset based on Hamming distance.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv1 (str): Path to the first input CSV file.\n",
    "    - input_csv2 (str): Path to the second input CSV file.\n",
    "    - output_csv (str): Path to the output CSV file.\n",
    "    - HD (int): Hamming distance threshold.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing the final collapsed barcodes.\n",
    "    '''\n",
    "    # Load input CSV files\n",
    "    df1 = pd.read_csv(input_csv1)\n",
    "    df2 = pd.read_csv(input_csv2)\n",
    "\n",
    "    # Extract barcodes from DataFrames\n",
    "    all_bcs_rep1 = sorted(df1['Barcode'])\n",
    "    all_bcs_rep2 = sorted(df2['Barcode'])\n",
    "\n",
    "    # Step 1: Collapse between datasets\n",
    "    bc_map = {}\n",
    "    for i, bc1 in enumerate(all_bcs_rep1):\n",
    "        if i > 0 and i % 500 == 0:\n",
    "            print(f'Mapped {i} out of {len(all_bcs_rep1)} barcodes')\n",
    "\n",
    "        mapped = False\n",
    "        for bc2 in all_bcs_rep2:\n",
    "            if hamming_distance(bc1, bc2) <= HD:\n",
    "                mapped = True\n",
    "                bc_map[bc1] = bc2\n",
    "                break\n",
    "\n",
    "        if not mapped:\n",
    "            bc_map[bc1] = bc1\n",
    "\n",
    "    print(f'\\nCollapsed between datasets: {len(bc_map)} barcodes')\n",
    "\n",
    "    # Step 2: Collapse within a dataset\n",
    "    for i, bc1 in enumerate(bc_map):\n",
    "        if i > 0 and i % 500 == 0:\n",
    "            print(f'Mapped {i} out of {len(bc_map)} barcodes')\n",
    "\n",
    "        mapped = False\n",
    "        for bc2 in bc_map:\n",
    "            if hamming_distance(bc1, bc2) <= HD:\n",
    "                mapped = True\n",
    "                bc_map[bc1] = bc2\n",
    "                break\n",
    "\n",
    "        if not mapped:\n",
    "            bc_map[bc1] = bc1\n",
    "\n",
    "    print(f'\\nFinal collapsed barcodes: {len(bc_map)} barcodes')\n",
    "\n",
    "\n",
    "    # Filter df1 by rows that contain barcodes from bc_map\n",
    "    df_filtered = df1[df1['Barcode'].isin(bc_map.keys())]\n",
    "\n",
    "    # Save the filtered DataFrame to an output CSV file\n",
    "    df_filtered.to_csv(output_csv, sep='\\t', index=False)\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# # Example usage:\n",
    "# input_csv1 = os.path.join(data_directory, 'Library_Test_R2_barcodes_whitelist.csv')\n",
    "# input_csv2 = os.path.join(data_directory, 'Library_Test_R1_barcodes_whitelist.csv')\n",
    "# output_csv = os.path.join(data_directory, 'whitelist_merged.csv')\n",
    "# HD = 1 # Set your Hamming distance threshold\n",
    "\n",
    "# result_dataframe = collapse_and_save(input_csv1, input_csv2, output_csv, HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_and_save_directory(input_directory, output_directory, HD):\n",
    "    '''\n",
    "    Collapse barcodes between datasets and within a dataset based on Hamming distance for all pairs of CSV files in a directory.\n",
    "\n",
    "    Parameters:\n",
    "    - input_directory (str): Path to the input directory containing pairs of CSV files.\n",
    "    - output_directory (str): Path to the output directory for saving collapsed CSV files.\n",
    "    - HD (int): Hamming distance threshold.\n",
    "\n",
    "    Returns:\n",
    "    - List of pd.DataFrame: List of DataFrames containing the final collapsed barcodes for each pair.\n",
    "    '''\n",
    "    result_dataframes = []\n",
    "\n",
    "    # List all files in the input directory\n",
    "    files = os.listdir(input_directory)\n",
    "\n",
    "    # Iterate through each file in the directory\n",
    "    for file in files:\n",
    "        # Check if the file has 'R1' in its name\n",
    "        if file.endswith('.csv') and 'duplicate_1' and 'whitelist' in file:\n",
    "            # Generate the corresponding file name for 'R2'\n",
    "            file_R2 = file.replace('duplicate_1', 'duplicate_2')\n",
    "\n",
    "            # Form the full paths for both R1 and R2 files\n",
    "            input_csv1 = os.path.join(input_directory, file)\n",
    "            input_csv2 = os.path.join(input_directory, file_R2)\n",
    "\n",
    "            # Extract the library name (assuming it is common between R1 and R2)\n",
    "            library_name = file.replace('_duplicate_1_', '_').replace('_duplicate_2_', '_').split('.csv')[0]\n",
    "            print(library_name)\n",
    "            # Form the output CSV file path\n",
    "            output_csv = os.path.join(output_directory, f'{library_name}_merged.csv')\n",
    "\n",
    "            # Perform barcode collapsing for the current pair\n",
    "            collapse_and_save(input_csv1, input_csv2, output_csv, HD)\n",
    "\n",
    "            # # Append the resulting DataFrame to the list\n",
    "            # result_dataframes.append(df_filtered)\n",
    "\n",
    "            # # Save the individual CSV file for the current pair\n",
    "            # individual_output_csv = os.path.join(output_directory, f'whitelist_{library_name}_individual.csv')\n",
    "            # df_filtered.to_csv(individual_output_csv, sep='\\t', index=False)\n",
    "\n",
    "    # return result_dataframes\n",
    "\n",
    "# Example usage:\n",
    "input_directory = data_directory\n",
    "output_directory = data_directory\n",
    "HD = 5  # Set your Hamming distance threshold\n",
    "\n",
    "result_dataframes = collapse_and_save_directory(input_directory, output_directory, HD)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Juypterenv)",
   "language": "python",
   "name": "juypterenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
